#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ Query Expansion –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö
–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ RAG –∏ RAG —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–æ–≤
"""

import os
import numpy as np
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from sentence_transformers import SentenceTransformer
import umap
from sklearn.metrics.pairwise import cosine_distances
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º backend –±–µ–∑ GUI
from openai import OpenAI

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
ORIGINAL_CHROMA_DB_PATH = "../02 Embeddings Data Retrieval/chroma_db_embeddings"
COLLECTION_NAME = "knowledge_base_embeddings"
N_RESULTS_PER_QUERY = 5
MAX_ALTERNATIVE_QUERIES = 5
OPENAI_BASE_URL = "http://127.0.0.1:1234/v1"
OPENAI_API_KEY = "dummy"
MODEL_NAME = "google/gemma-3-12b"

def initialize_chromadb():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
    try:
        client = chromadb.PersistentClient(path=ORIGINAL_CHROMA_DB_PATH)
        embedding_function = SentenceTransformerEmbeddingFunction()
        collection = client.get_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        return client, collection
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ChromaDB: {e}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–ø—É—â–µ–Ω –º–æ–¥—É–ª—å '02 Embeddings Data Retrieval' –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        return None, None

def search_documents(collection, query: str, n_results: int = N_RESULTS_PER_QUERY):
    """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    try:
        results = collection.query(
            query_texts=[query],
            n_results=n_results,
            include=["documents", "metadatas", "embeddings"]
        )
        return results
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return None

def augment_multiple_query(query, model=MODEL_NAME):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM"""
    openai_client = OpenAI(
        base_url=OPENAI_BASE_URL,
        api_key=OPENAI_API_KEY
    )
    
    messages = [
        {
            "role": "system",
            "content": "–í—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫-—ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—é—Ç –≤–æ–ø—Ä–æ—Å—ã –æ–± –≥–æ–¥–æ–≤–æ–º –æ—Ç—á–µ—Ç–µ. "
            "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –¥–æ –ø—è—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç –∏–º –Ω–∞–π—Ç–∏ –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. "
            "–ü—Ä–µ–¥–ª–∞–≥–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –∫–æ—Ä–æ—Ç–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–º—ã. "
            "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–æ –ø–æ–ª–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º. "
            "–í—ã–≤–æ–¥–∏—Ç–µ –ø–æ –æ–¥–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É –Ω–∞ —Å—Ç—Ä–æ–∫—É. –ù–µ –Ω—É–º–µ—Ä—É–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã."
        },
        {"role": "user", "content": query}
    ]

    try:
        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
        )
        content = response.choices[0].message.content
        questions = [q.strip() for q in content.split("\n") if q.strip()]
        return questions[:MAX_ALTERNATIVE_QUERIES]
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
        return []

def deduplicate_documents(all_documents, all_metadatas):
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
    seen_content = set()
    unique_documents = []
    unique_metadatas = []
    
    for doc, meta in zip(all_documents, all_metadatas):
        if doc not in seen_content:
            seen_content.add(doc)
            unique_documents.append(doc)
            unique_metadatas.append(meta)
    
    return unique_documents, unique_metadatas

def get_all_embeddings(collection):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö embeddings –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
    results = collection.get(
        include=["embeddings", "documents", "metadatas"]
    )
    return results

def calculate_distances(query_embedding, document_embeddings):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É query –∏ document embeddings"""
    distances = []
    for doc_emb in document_embeddings:
        dist = cosine_distances([query_embedding], [doc_emb])[0][0]
        distances.append(dist)
    return distances

def create_visualization(embeddings_2d, labels, distances=None, query_text="", 
                        alternative_queries=None, visualization_type="basic", 
                        found_by_query=None):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é matplotlib"""
    plt.figure(figsize=(14, 10))
    
    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–æ—á–∫–∏ –ø–æ —Ç–∏–ø–∞–º
    query_indices = [i for i, label in enumerate(labels) if label == "QUERY"]
    doc_indices = [i for i, label in enumerate(labels) if label == "DOC"]
    found_doc_indices = [i for i, label in enumerate(labels) if label == "FOUND_DOC"]
    
    # –î–ª—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
    all_x = [point[0] for point in embeddings_2d]
    all_y = [point[1] for point in embeddings_2d]
    
    # –î–æ–∫—É–º–µ–Ω—Ç—ã (—Ç–µ–º–Ω–æ-—Å–∏–Ω–∏–µ —Ç–æ—á–∫–∏)
    if doc_indices:
        doc_x = [all_x[i] for i in doc_indices]
        doc_y = [all_y[i] for i in doc_indices]
        plt.scatter(doc_x, doc_y, c='darkblue', alpha=0.6, s=50, label='–î–æ–∫—É–º–µ–Ω—Ç—ã', marker='o')
    
    # –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ü–≤–µ—Ç–æ–≤–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
    if found_doc_indices:
        found_x = [all_x[i] for i in found_doc_indices]
        found_y = [all_y[i] for i in found_doc_indices]
        
        if visualization_type == "expanded" and found_by_query:
            # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏
            colors = ['orange', 'green', 'purple', 'brown', 'pink', 'gray']
            for query_idx, doc_indices_for_query in found_by_query.items():
                if doc_indices_for_query:
                    color = colors[query_idx % len(colors)]
                    query_name = f"–ó–∞–ø—Ä–æ—Å {query_idx + 1}" if query_idx > 0 else "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å"
                    
                    x_coords = [found_x[i] for i in doc_indices_for_query if i < len(found_x)]
                    y_coords = [found_y[i] for i in doc_indices_for_query if i < len(found_y)]
                    
                    if x_coords and y_coords:
                        sizes = [80 + (1-distances[i])*120 if distances and i < len(distances) else 100 
                                for i in doc_indices_for_query if i < len(found_x)]
                        plt.scatter(x_coords, y_coords, c=color, alpha=0.8, s=sizes, 
                                  label=query_name, marker='o')
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–æ—Ä–∞–Ω–∂–µ–≤—ã–µ —Ç–æ—á–∫–∏)
            if distances and len(distances) >= len(found_doc_indices):
                sizes = [80 + (1-dist)*120 for dist in distances[:len(found_doc_indices)]]
            else:
                sizes = [100] * len(found_doc_indices)
            plt.scatter(found_x, found_y, c='orange', alpha=0.8, s=sizes, 
                       label='–ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã', marker='o')
    
    # –ó–∞–ø—Ä–æ—Å (–∫—Ä–∞—Å–Ω–∞—è –∑–≤–µ–∑–¥–∞)
    if query_indices:
        query_x = [all_x[i] for i in query_indices]
        query_y = [all_y[i] for i in query_indices]
        plt.scatter(query_x, query_y, c='red', alpha=1.0, s=400, label='–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å', 
                   marker='*', edgecolors='black', linewidth=2)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    if query_indices:
        for i in query_indices:
            plt.annotate('–ó–ê–ü–†–û–°', (all_x[i], all_y[i]), 
                        xytext=(15, 15), textcoords='offset points',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.7),
                        fontsize=10, ha='left', fontweight='bold')
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    if visualization_type == "basic":
        title = f'–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RAG\n–ó–∞–ø—Ä–æ—Å: "{query_text}"'
        subtitle = f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(found_doc_indices)}"
    else:
        title = f'RAG —Å Query Expansion\n–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: "{query_text}"'
        alt_queries_text = ""
        if alternative_queries:
            alt_queries_text = f"\n–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {len(alternative_queries)}"
        subtitle = f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(found_doc_indices)}{alt_queries_text}"
    
    plt.xlabel('UMAP Dimension 1', fontsize=12)
    plt.ylabel('UMAP Dimension 2', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.text(0.02, 0.98, subtitle, transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    plt.legend(fontsize=10, loc='upper right')
    plt.grid(True, alpha=0.3)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    filename = f'query_expansion_{visualization_type}.png'
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üíæ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ '{filename}'")
    return filename

def visualize_basic_rag(collection, query):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ RAG"""
    print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ RAG...")
    
    # –ü–æ–ª—É—á–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    model = SentenceTransformer('all-MiniLM-L6-v2')
    query_embedding = model.encode([query])[0]
    
    # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    results = search_documents(collection, query, N_RESULTS_PER_QUERY)
    if not results:
        return None
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ embeddings –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    all_data = get_all_embeddings(collection)
    all_embeddings = all_data['embeddings']
    all_documents = all_data['documents']
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è UMAP
    embeddings_for_umap = [query_embedding] + all_embeddings
    labels = ["QUERY"] + ["DOC"] * len(all_embeddings)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º UMAP
    reducer = umap.UMAP(n_components=2, random_state=42, transform_seed=42)
    embeddings_2d = reducer.fit_transform(embeddings_for_umap)
    
    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    found_docs_content = results['documents'][0]
    found_indices = []
    for found_doc in found_docs_content:
        try:
            doc_index = all_documents.index(found_doc)
            found_indices.append(doc_index)
        except ValueError:
            continue
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    visual_labels = ["QUERY"] + ["DOC"] * len(all_embeddings)
    for idx in found_indices:
        if idx < len(visual_labels) - 1:
            visual_labels[idx + 1] = "FOUND_DOC"
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    found_embeddings = results['embeddings'][0]
    distances = calculate_distances(query_embedding, found_embeddings)
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    filename = create_visualization(embeddings_2d, visual_labels, distances=distances, 
                                  query_text=query, visualization_type="basic")
    
    return {
        'results': results,
        'distances': distances,
        'filename': filename,
        'embeddings_2d': embeddings_2d,
        'reducer': reducer,
        'query_embedding': query_embedding,
        'all_data': all_data
    }

def visualize_expanded_rag(collection, query, basic_data=None):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å Query Expansion"""
    print(f"\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ RAG —Å Query Expansion...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if basic_data:
        query_embedding = basic_data['query_embedding']
        all_data = basic_data['all_data']
        reducer = basic_data['reducer']
        embeddings_2d = basic_data['embeddings_2d']
    else:
        # –ü–æ–ª—É—á–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        model = SentenceTransformer('all-MiniLM-L6-v2')
        query_embedding = model.encode([query])[0]
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ embeddings –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        all_data = get_all_embeddings(collection)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è UMAP
        all_embeddings = all_data['embeddings']
        embeddings_for_umap = [query_embedding] + all_embeddings
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º UMAP
        reducer = umap.UMAP(n_components=2, random_state=42, transform_seed=42)
        embeddings_2d = reducer.fit_transform(embeddings_for_umap)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    alternative_queries = augment_multiple_query(query)
    if not alternative_queries:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
        return None
    
    print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(alternative_queries)} –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    all_queries = [query] + alternative_queries
    all_documents_found = []
    all_metadatas_found = []
    found_by_query = {}  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º, –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞–π–¥–µ–Ω—ã –∫–∞–∫–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
    
    for i, search_query in enumerate(all_queries):
        results = search_documents(collection, search_query, N_RESULTS_PER_QUERY)
        if results:
            query_docs = results['documents'][0]
            query_metas = results['metadatas'][0]
            
            # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            start_idx = len(all_documents_found)
            all_documents_found.extend(query_docs)
            all_metadatas_found.extend(query_metas)
            end_idx = len(all_documents_found)
            
            found_by_query[i] = list(range(start_idx, end_idx))
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    unique_documents, unique_metadatas = deduplicate_documents(all_documents_found, all_metadatas_found)
    
    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ–±—â–µ–º –º–∞—Å—Å–∏–≤–µ
    all_documents = all_data['documents']
    found_indices = []
    for unique_doc in unique_documents:
        try:
            doc_index = all_documents.index(unique_doc)
            found_indices.append(doc_index)
        except ValueError:
            continue
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    visual_labels = ["QUERY"] + ["DOC"] * len(all_documents)
    for idx in found_indices:
        if idx < len(visual_labels) - 1:
            visual_labels[idx + 1] = "FOUND_DOC"
    
    # –ü–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –≤—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    unique_embeddings = []
    for unique_doc in unique_documents:
        try:
            doc_index = all_documents.index(unique_doc)
            unique_embeddings.append(all_data['embeddings'][doc_index])
        except ValueError:
            continue
    
    distances = calculate_distances(query_embedding, unique_embeddings)
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    filename = create_visualization(embeddings_2d, visual_labels, distances=distances, 
                                  query_text=query, alternative_queries=alternative_queries,
                                  visualization_type="expanded", found_by_query=found_by_query)
    
    return {
        'unique_documents': unique_documents,
        'alternative_queries': alternative_queries,
        'distances': distances,
        'filename': filename,
        'total_found': len(all_documents_found),
        'unique_found': len(unique_documents)
    }

def compare_visualizations(basic_result, expanded_result, query):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
    print(f"\nüìã –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
    
    if not basic_result or not expanded_result:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è side-by-side —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ –≤—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    
    plt.suptitle(f'–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ RAG\n–ó–∞–ø—Ä–æ—Å: "{query}"', fontsize=16, fontweight='bold')
    
    # –õ–µ–≤–∞—è –ø–∞–Ω–µ–ª—å - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ RAG
    ax1.text(0.1, 0.9, "üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RAG", fontsize=14, fontweight='bold', transform=ax1.transAxes)
    ax1.text(0.1, 0.8, f"‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(basic_result['results']['documents'][0])}", 
             fontsize=12, transform=ax1.transAxes)
    ax1.text(0.1, 0.7, f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.mean(basic_result['distances']):.3f}", 
             fontsize=12, transform=ax1.transAxes)
    ax1.text(0.1, 0.6, f"‚Ä¢ –ú–∏–Ω. —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {min(basic_result['distances']):.3f}", 
             fontsize=12, transform=ax1.transAxes)
    ax1.text(0.1, 0.5, f"‚Ä¢ –ú–∞–∫—Å. —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {max(basic_result['distances']):.3f}", 
             fontsize=12, transform=ax1.transAxes)
    
    # –ü—Ä–∞–≤–∞—è –ø–∞–Ω–µ–ª—å - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ RAG
    ax2.text(0.1, 0.9, "üìà RAG —Å Query Expansion", fontsize=14, fontweight='bold', transform=ax2.transAxes)
    ax2.text(0.1, 0.8, f"‚Ä¢ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(expanded_result['alternative_queries'])}", 
             fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.7, f"‚Ä¢ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {expanded_result['total_found']}", 
             fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.6, f"‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {expanded_result['unique_found']}", 
             fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.5, f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.mean(expanded_result['distances']):.3f}", 
             fontsize=12, transform=ax2.transAxes)
    
    # –£–±–∏—Ä–∞–µ–º –æ—Å–∏
    ax1.set_xticks([])
    ax1.set_yticks([])
    ax2.set_xticks([])
    ax2.set_yticks([])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    filename = 'query_expansion_comparison.png'
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üíæ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ '{filename}'")
    return filename

def visualize_query_expansion(query):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π Query Expansion"""
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π Query Expansion")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
    print("\nüóÑÔ∏è –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    client, collection = initialize_chromadb()
    
    if not collection:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return None
    
    print("‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–∏–ª–∏—Å—å –∫ ChromaDB")
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ RAG
    basic_result = visualize_basic_rag(collection, query)
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ RAG
    expanded_result = visualize_expanded_rag(collection, query, basic_result)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    if basic_result and expanded_result:
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò")
        print("=" * 60)
        print(f"üîç –ó–∞–ø—Ä–æ—Å: {query}")
        print(f"üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RAG: {len(basic_result['results']['documents'][0])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"üìà Query Expansion: {expanded_result['unique_found']} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        improvement = expanded_result['unique_found'] / len(basic_result['results']['documents'][0])
        print(f"üìà –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.1f}x –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"üíæ –°–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã:")
        print(f"   ‚Ä¢ {basic_result['filename']}")
        print(f"   ‚Ä¢ {expanded_result['filename']}")
    
    return {
        'basic': basic_result,
        'expanded': expanded_result,
        'query': query
    }

if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    query = "–ö–∞–∫–∏–µ –ø–ª–∞–Ω—ã –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ò–ò?"
    visualize_query_expansion(query)
