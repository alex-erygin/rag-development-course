#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É embeddings –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
"""

import os
import numpy as np
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from sentence_transformers import SentenceTransformer
import umap
from sklearn.metrics.pairwise import cosine_distances
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º backend –±–µ–∑ GUI

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
CHROMA_DB_PATH = "../02 Embeddings Data Retrieval/chroma_db_embeddings"
COLLECTION_NAME = "knowledge_base_embeddings"

def load_text_files(directory: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    import glob
    text_files = glob.glob(os.path.join(directory, "*.md"))
    documents = []
    
    for file_path in text_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read().strip()
                if content:
                    documents.append({
                        'content': content,
                        'filename': os.path.basename(file_path)
                    })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
    
    return documents

def split_documents_into_chunks(documents: list, chunk_size: int = 256) -> list:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
    from langchain.text_splitter import SentenceTransformersTokenTextSplitter
    token_splitter = SentenceTransformersTokenTextSplitter(
        chunk_overlap=0, 
        tokens_per_chunk=chunk_size
    )
    
    chunks = []
    for doc in documents:
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏
        doc_chunks = token_splitter.split_text(doc['content'])
        for i, chunk in enumerate(doc_chunks):
            chunks.append({
                'content': chunk,
                'source_file': doc['filename'],
                'chunk_index': i
            })
    
    return chunks

def add_chunks_to_chromadb(collection, chunks: list):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ ChromaDB"""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    documents = [chunk['content'] for chunk in chunks]
    ids = [f"chunk_{i}" for i in range(len(chunks))]
    metadatas = [
        {
            'source_file': chunk['source_file'],
            'chunk_index': chunk['chunk_index']
        } 
        for chunk in chunks
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection.add(
        documents=documents,
        ids=ids,
        metadatas=metadatas
    )
    
    print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")

def connect_to_chromadb():
    """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π ChromaDB"""
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    try:
        collection = client.get_collection(name=COLLECTION_NAME)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if collection.count() == 0:
            print("–ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏ KB
            kb_directory = os.path.join("..", "02 Embeddings Data Retrieval", "KB")
            documents = load_text_files(kb_directory)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = split_documents_into_chunks(documents, 256)
            print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            if chunks:
                add_chunks_to_chromadb(collection, chunks)
            else:
                print("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é")
        else:
            print(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç {collection.count()} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return client, collection
    except Exception as e:
        print(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {COLLECTION_NAME} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é...")
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
        embedding_function = SentenceTransformerEmbeddingFunction()
        collection = client.create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏ KB
        kb_directory = os.path.join("..", "02 Embeddings Data Retrieval", "KB")
        documents = load_text_files(kb_directory)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = split_documents_into_chunks(documents, 256)
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
        if chunks:
            add_chunks_to_chromadb(collection, chunks)
        else:
            print("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é")
        
        return client, collection

def get_all_embeddings(collection):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö embeddings –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∏—Ö embeddings
    results = collection.get(
        include=["embeddings", "documents", "metadatas"]
    )
    return results

def calculate_distances(query_embedding, document_embeddings):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É query –∏ document embeddings"""
    distances = []
    for doc_emb in document_embeddings:
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        dist = cosine_distances([query_embedding], [doc_emb])[0][0]
        distances.append(dist)
    return distances

def create_matplotlib_visualization(embeddings_2d, labels, distances=None, query_text="", visualization_type="all"):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Å–∏–≤–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é matplotlib"""
    plt.figure(figsize=(12, 8))
    
    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–æ—á–∫–∏ –ø–æ —Ç–∏–ø–∞–º
    query_indices = [i for i, label in enumerate(labels) if label == "QUERY"]
    doc_indices = [i for i, label in enumerate(labels) if label == "DOC"]
    found_doc_indices = [i for i, label in enumerate(labels) if label == "FOUND_DOC"]
    
    # –î–ª—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
    all_x = [point[0] for point in embeddings_2d]
    all_y = [point[1] for point in embeddings_2d]
    
    # –°–æ–∑–¥–∞–µ–º scatter plot
    # –î–æ–∫—É–º–µ–Ω—Ç—ã (—Ç–µ–º–Ω–æ-—Å–∏–Ω–∏–µ —Ç–æ—á–∫–∏)
    if doc_indices:
        doc_x = [all_x[i] for i in doc_indices]
        doc_y = [all_y[i] for i in doc_indices]
        plt.scatter(doc_x, doc_y, c='darkblue', alpha=0.6, s=50, label='–î–æ–∫—É–º–µ–Ω—Ç—ã', marker='o')
    
    # –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–æ—Ä–∞–Ω–∂–µ–≤—ã–µ –∫—Ä—É–∂–æ—á–∫–∏)
    if found_doc_indices:
        found_x = [all_x[i] for i in found_doc_indices]
        found_y = [all_y[i] for i in found_doc_indices]
        # –†–∞–∑–º–µ—Ä —Ç–æ—á–µ–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (–æ–±—Ä–∞—Ç–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å) - —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
        if distances and len(distances) >= len(found_doc_indices):
            sizes = [30 + (1-dist)*100 for dist in distances[:len(found_doc_indices)]]
        else:
            sizes = [100] * len(found_doc_indices)
        plt.scatter(found_x, found_y, c='orange', alpha=0.8, s=sizes, label='–ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã', marker='o')
    
    # –ó–∞–ø—Ä–æ—Å (–∫—Ä–∞—Å–Ω–∞—è —Ç–æ—á–∫–∞)
    if query_indices:
        query_x = [all_x[i] for i in query_indices]
        query_y = [all_y[i] for i in query_indices]
        plt.scatter(query_x, query_y, c='red', alpha=1.0, s=300, label='–ó–∞–ø—Ä–æ—Å', marker='*', edgecolors='black', linewidth=2)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∫ —Ç–æ—á–∫–∞–º –∑–∞–ø—Ä–æ—Å–∞ –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    if query_indices:
        for i in query_indices:
            plt.annotate('–ó–ê–ü–†–û–°', (all_x[i], all_y[i]), 
                        xytext=(10, 10), textcoords='offset points',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.7),
                        fontsize=10, ha='left', fontweight='bold')
    
    if found_doc_indices and distances:
        for i, (idx, dist) in enumerate(zip(found_doc_indices, distances)):
            if i < len(distances):
                plt.annotate(f'–î–æ–∫—É–º–µ–Ω—Ç {i+1}\n–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {dist:.3f}', 
                           (all_x[idx], all_y[idx]),
                           xytext=(10, 10), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7),
                           fontsize=9, ha='left')
    
    plt.xlabel('UMAP Dimension 1', fontsize=12)
    plt.ylabel('UMAP Dimension 2', fontsize=12)
    plt.title(f'–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Embeddings (UMAP)\n–ó–∞–ø—Ä–æ—Å: "{query_text}"', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º
    filename = f'vector_search_visualization_{visualization_type}.png'
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üíæ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ '{filename}'")

def print_distance_info(distances, labels):
    """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö"""
    print("\n" + "=" * 60)
    print("–†–ê–°–°–¢–û–Ø–ù–ò–Ø –ú–ï–ñ–î–£ –ó–ê–ü–†–û–°–û–ú –ò –î–û–ö–£–ú–ï–ù–¢–ê–ú–ò")
    print("=" * 60)
    print(f"üéØ –ó–ê–ü–†–û–°: –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("-" * 50)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é (–æ—Ç –±–ª–∏–∂–∞–π—à–µ–≥–æ –∫ –¥–∞–ª—å–Ω–µ–º—É)
    sorted_distances = sorted(enumerate(distances), key=lambda x: x[1])
    
    for i, (doc_idx, dist) in enumerate(sorted_distances):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –±–ª–∏–∑–æ—Å—Ç–∏
        if dist < 0.3:
            proximity = "üî• –û–ß–ï–ù–¨ –ë–õ–ò–ó–ö–û"
        elif dist < 0.5:
            proximity = "üëç –ë–õ–ò–ó–ö–û"
        elif dist < 0.7:
            proximity = "üëå –°–†–ï–î–ù–ï"
        else:
            proximity = "‚ùÑÔ∏è –î–ê–õ–ï–ö–û"
            
        print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç {doc_idx}: –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ = {dist:.4f} ({proximity})")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å –∏–∫–æ–Ω–∫–∞–º–∏
    if distances:
        print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ê–°–°–¢–û–Ø–ù–ò–ô:")
        print(f"  üìä –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {min(distances):.4f}")
        print(f"  üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {max(distances):.4f}")
        print(f"  ‚öñÔ∏è  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(distances):.4f}")
        print(f"  üìê –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(distances):.4f}")

def search_and_visualize(collection, query: str, n_results: int = 5):
    """–ü–æ–∏—Å–∫ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    
    # –ü–æ–ª—É—á–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    model = SentenceTransformer('all-MiniLM-L6-v2')
    query_embedding = model.encode([query])[0]
    
    # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    results = collection.query(
        query_texts=[query],
        n_results=n_results,
        include=["documents", "metadatas", "embeddings"]
    )
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ embeddings –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    all_data = get_all_embeddings(collection)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è UMAP
    all_embeddings = all_data['embeddings']
    all_documents = all_data['documents']
    
    # –î–æ–±–∞–≤–ª—è–µ–º query embedding –∫ –¥–∞–Ω–Ω—ã–º
    embeddings_for_umap = [query_embedding] + all_embeddings
    labels = ["QUERY"] + ["DOC"] * len(all_embeddings)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º UMAP –¥–ª—è –ø–æ–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–æ 2D
    print("üìê –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ UMAP –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
    reducer = umap.UMAP(n_components=2, random_state=42)
    embeddings_2d = reducer.fit_transform(embeddings_for_umap)
    
    # –°–æ–∑–¥–∞–µ–º matplotlib –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
    print("üé® –°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Å–∏–≤–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö embeddings...")
    create_matplotlib_visualization(embeddings_2d, labels, query_text=query, visualization_type="all")
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    found_embeddings = results['embeddings'][0]
    distances = calculate_distances(query_embedding, found_embeddings)
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –í–°–ï–• –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ embeddings –∏ –ø–æ–º–µ—á–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    all_labels = ["QUERY"] + ["DOC"] * len(all_embeddings)  # –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ
    
    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ–±—â–µ–º –º–∞—Å—Å–∏–≤–µ
    found_indices = []
    found_docs_content = results['documents'][0]
    for found_doc in found_docs_content:
        try:
            doc_index = all_documents.index(found_doc)
            found_indices.append(doc_index)
        except ValueError:
            # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
            continue
    
    # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    found_labels = ["QUERY"] + ["DOC"] * len(all_embeddings)
    # –ü–æ–º–µ—á–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∫–∞–∫ FOUND_DOC
    for idx in found_indices:
        if idx < len(found_labels) - 1:  # -1 –ø–æ—Ç–æ–º—É —á—Ç–æ –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç - QUERY
            found_labels[idx + 1] = "FOUND_DOC"  # +1 –ø–æ—Ç–æ–º—É —á—Ç–æ –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç - QUERY
    
    # –°–æ–∑–¥–∞–µ–º matplotlib –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö
    print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    create_matplotlib_visualization(embeddings_2d, found_labels, distances=distances, query_text=query, visualization_type="found")
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ found_labels –¥–ª—è –≤—ã–≤–æ–¥–∞)
    found_labels_for_print = ["QUERY"] + ["FOUND_DOC"] * len(found_embeddings)
    print_distance_info(distances, found_labels_for_print)
    
    return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞"""
    print("üìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É embeddings")
    
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB
        print("\nüóÑÔ∏è –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π ChromaDB...")
        client, collection = connect_to_chromadb()
        print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {COLLECTION_NAME}")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        query = "–ö–∞–∫–æ–≤–∞ —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å –°–±–µ—Ä–±–∞–Ω–∫–∞ –≤ 2024 –≥–æ–¥—É?"
        print(f"\n‚ùì –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}")
        
        # –ü–æ–∏—Å–∫ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        results = search_and_visualize(collection, query, n_results=5)
        
        # –í—ã–≤–æ–¥ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print("\n" + "=" * 60)
        print("–ù–ê–ô–î–ï–ù–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´")
        print("=" * 60)
        
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        
        for i, (doc, meta) in enumerate(zip(documents, metadatas), 1):
            print(f"\n{i}. –§–∞–π–ª: {meta['source_file']}, –ß–∞–Ω–∫: {meta['chunk_index']}")
            print(f"   –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ ChromaDB —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")

if __name__ == "__main__":
    main()
