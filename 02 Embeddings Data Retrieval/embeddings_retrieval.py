#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å embeddings –∏ RAG (Retrieval-Augmented Generation)
"""

import os
import glob
from pypdf import PdfReader
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
import openai
from openai import OpenAI

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
import os
KB_DIRECTORY = os.path.join(os.path.dirname(__file__), "KB")
CHROMA_DB_PATH = "./chroma_db_embeddings"
COLLECTION_NAME = "knowledge_base_embeddings"
CHUNK_SIZE_TOKENS = 256

def load_text_files(directory: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    text_files = glob.glob(os.path.join(directory, "*.md"))
    documents = []
    
    for file_path in text_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read().strip()
                if content:
                    documents.append({
                        'content': content,
                        'filename': os.path.basename(file_path)
                    })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
    
    return documents

def split_documents_into_chunks(documents: list, chunk_size: int = CHUNK_SIZE_TOKENS) -> list:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
    token_splitter = SentenceTransformersTokenTextSplitter(
        chunk_overlap=0, 
        tokens_per_chunk=chunk_size
    )
    
    chunks = []
    for doc in documents:
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏
        doc_chunks = token_splitter.split_text(doc['content'])
        for i, chunk in enumerate(doc_chunks):
            chunks.append({
                'content': chunk,
                'source_file': doc['filename'],
                'chunk_index': i
            })
    
    return chunks

def initialize_chromadb():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç ChromaDB
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # –£–¥–∞–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    try:
        client.delete_collection(COLLECTION_NAME)
    except:
        pass
    
    # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    embedding_function = SentenceTransformerEmbeddingFunction()
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection = client.create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_function
    )
    
    return client, collection

def add_chunks_to_chromadb(collection, chunks: list):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ ChromaDB"""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    documents = [chunk['content'] for chunk in chunks]
    ids = [f"chunk_{i}" for i in range(len(chunks))]
    metadatas = [
        {
            'source_file': chunk['source_file'],
            'chunk_index': chunk['chunk_index']
        } 
        for chunk in chunks
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection.add(
        documents=documents,
        ids=ids,
        metadatas=metadatas
    )
    
    print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")

def search_documents(collection, query: str, n_results: int = 5):
    """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    results = collection.query(
        query_texts=[query],
        n_results=n_results,
        include=["documents", "metadatas"]
    )
    
    return results

def print_search_results(results):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–Ω—Å–æ–ª—å"""
    print("=" * 60)
    print("–ù–ê–ô–î–ï–ù–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´")
    print("=" * 60)
    
    documents = results['documents'][0]
    metadatas = results['metadatas'][0]
    
    for i, (doc, meta) in enumerate(zip(documents, metadatas), 1):
        print(f"\n{i}. –§–∞–π–ª: {meta['source_file']}, –ß–∞–Ω–∫: {meta['chunk_index']}")
        print(f"   –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc}")

def rag(query, retrieved_documents, model="google/gemma-3-12b"):
    """–§—É–Ω–∫—Ü–∏—è RAG –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    information = "\n\n".join(retrieved_documents)

    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç OpenAI —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    openai_client = OpenAI(
        base_url="http://127.0.0.1:1234/v1",
        api_key="dummy"
    )
    
    messages = [
        {
            "role": "system",
            "content": "–í—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫-—ç–∫—Å–ø–µ—Ä—Ç. –í–∞—à–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—é—Ç –≤–æ–ø—Ä–æ—Å—ã –æ–± –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π—Å—è –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."
            "–í–∞–º –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
        },
        {"role": "user", "content": f"Question: {query}. \n Information: {information}"}
    ]
    
    try:
        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
        )
        content = response.choices[0].message.content
        return content
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LLM: {e}")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM"

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é —Ä–∞–±–æ—Ç—ã —Å embeddings –∏ RAG")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
    print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    documents = load_text_files(KB_DIRECTORY)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # 2. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
    print("\n‚úÇÔ∏è –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = split_documents_into_chunks(documents, CHUNK_SIZE_TOKENS)
    print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    
    # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
    print("\nüóÑÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB...")
    client, collection = initialize_chromadb()
    
    # 4. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ ChromaDB
    print("\n‚ûï –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
    add_chunks_to_chromadb(collection, chunks)
    
    # 5. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    query = "–ö–∞–∫–æ–≤–∞ —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å –°–±–µ—Ä–±–∞–Ω–∫–∞ –≤ 2024 –≥–æ–¥—É?"
    print(f"\n‚ùì –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}")
    
    # 6. –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
    print("\nüîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    results = search_documents(collection, query, n_results=5)
    
    # 7. –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö 5 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    print_search_results(results)
    
    # 8. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è LLM –∏ –≤—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞
    print("\nü§ñ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM...")
    retrieved_documents = results['documents'][0]
    response = rag(query, retrieved_documents)
    
    print("\n" + "=" * 60)
    print("–û–¢–í–ï–¢ –û–¢ LLM")
    print("=" * 60)
    print(response)

if __name__ == "__main__":
    main()
