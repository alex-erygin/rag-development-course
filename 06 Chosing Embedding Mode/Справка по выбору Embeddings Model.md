# Урок: Выбор подходящей модели Embeddings для RAG-систем

## Введение

Embedding-модели являются критически важным компонентом RAG-систем, преобразуя текст в векторные представления для эффективного семантического поиска. Правильный выбор модели напрямую влияет на качество извлечения релевантной информации и, как следствие, на качество финальных ответов системы.

## 1. Основные критерии выбора Embedding-модели

### 1.1. Ключевые параметры оценки

- **Качество векторных представлений** - способность модели улавливать семантическое сходство
- **Размер контекстного окна** - максимальная длина обрабатываемого текста
- **Языковая поддержка** - количество и качество поддерживаемых языков
- **Производительность** - скорость обработки и размер модели
- **Стоимость** - для коммерческих API или вычислительные ресурсы для self-hosted решений

## 2. Поддержка многоязычности

### 2.1. Классификация моделей по языковой поддержке

#### Монолингвальные модели (English-only)
- **Преимущества**: Высокое качество для английского языка, меньший размер
- **Недостатки**: Ограниченность применения
- **Примеры**: 
  - BGE-small-en-v1.5
  - all-MiniLM-L6-v2

#### Мультилингвальные модели
- **Преимущества**: Работа с множеством языков, кросс-языковой поиск
- **Недостатки**: Больший размер, возможное снижение качества для отдельных языков
- **Примеры**:
  - multilingual-e5-large (100+ языков)
  - LaBSE (109 языков)
  - mUSE (16 языков)

### 2.2. Топ мультилингвальных моделей

#### **1. Multilingual-E5 Series (Microsoft)**
- **Языки**: 100+
- **Размеры**: small (118M), base (278M), large (560M)
- **Особенности**: Отличный баланс качества и производительности
- **Контекст**: до 512 токенов

#### **2. BGE-M3 (BAAI)**
- **Языки**: 100+
- **Размер**: 1.2B параметров
- **Особенности**: Поддержка длинных контекстов (до 8192 токенов)
- **Уникальность**: Hybrid search (dense + sparse retrieval)

#### **3. OpenAI text-embedding-3**
- **Языки**: Широкая поддержка основных языков
- **Варианты**: 
  - text-embedding-3-small (размерность 512-1536)
  - text-embedding-3-large (размерность 256-3072)
- **Контекст**: до 8191 токенов

#### **4. Cohere Embed v3**
- **Языки**: 100+
- **Особенности**: Специализированные режимы для поиска и классификации
- **Контекст**: до 512 токенов

## 3. Результаты бенчмарков

### 3.1. MTEB (Massive Text Embedding Benchmark)

#### Что такое MTEB?

**MTEB (Massive Text Embedding Benchmark)** - это комплексный бенчмарк для оценки качества text embedding моделей, разработанный компанией Hugging Face. Это наиболее полный и авторитетный стандарт в индустрии для сравнения embedding-моделей.

**Ключевые характеристики:**
- **112+ языков** - включая редкие и низкоресурсные языки
- **8 категорий задач** - охватывающих различные аспекты работы с текстом
- **58+ датасетов** - разнообразные домены и типы данных
- **Открытый стандарт** - воспроизводимая методология оценки
- **Постоянное обновление** - регулярное добавление новых датасетов

#### 3.1.1. Восемь типов задач MTEB

MTEB оценивает модели по восьми категориям задач, каждая из которых отражает реальные сценарии использования embedding-моделей:

##### 1. **Retrieval (Информационный поиск)**
- **Описание**: Поиск релевантных документов по запросу - ключевая задача для RAG-систем
- **Метрика**: nDCG@10 (Normalized Discounted Cumulative Gain)
- **Примеры датасетов**: 
  - MSMARCO - поисковые запросы Microsoft Bing
  - NFCorpus - медицинские документы
  - HotpotQA - мультихоповый вопрос-ответ
  - ArguAna - аргументированный поиск
- **Важность для RAG**: Критически важна - напрямую влияет на качество извлечения контекста
- **Что оценивается**: Способность модели находить семантически релевантные документы среди большого корпуса

##### 2. **Semantic Textual Similarity (Семантическое сходство текстов)**
- **Описание**: Оценка степени семантической близости между парами предложений
- **Метрика**: Spearman correlation (корреляция Спирмена)
- **Примеры датасетов**:
  - STS Benchmark - разнообразные пары предложений
  - SICK-R - предложения с логическими отношениями
  - STS-B - многодоменные пары
- **Важность для RAG**: Высокая - помогает определить релевантность фрагментов
- **Что оценивается**: Точность модели в определении степени семантического сходства

##### 3. **Classification (Классификация)**
- **Описание**: Категоризация текстов по предопределенным классам
- **Метрика**: Accuracy (точность)
- **Примеры датасетов**:
  - Banking77 - классификация банковских запросов (77 категорий)
  - AmazonReviewsClassification - анализ отзывов
  - ToxicConversations - определение токсичности
  - EmotionClassification - распознавание эмоций
- **Важность для RAG**: Средняя - полезна для фильтрации и категоризации документов
- **Что оценивается**: Качество векторных представлений для задач классификации

##### 4. **Clustering (Кластеризация)**
- **Описание**: Группировка похожих текстов без предопределенных меток
- **Метрика**: V-measure (гармоническое среднее полноты и однородности)
- **Примеры датасетов**:
  - ArXiv Clustering - группировка научных статей по темам
  - BiorXiv Clustering - биомедицинские публикации
  - StackExchange Clustering - группировка вопросов по темам
  - TwentyNewsgroups - классические новостные группы
- **Важность для RAG**: Средняя - используется для организации базы знаний
- **Что оценивается**: Способность создавать компактные кластеры похожих документов

##### 5. **Pair Classification (Бинарная классификация пар)**
- **Описание**: Определение отношений между парами текстов (дубликаты, перефразирование и т.д.)
- **Метрика**: Average Precision (средняя точность)
- **Примеры датасетов**:
  - SprintDuplicateQuestions - поиск дубликатов вопросов
  - TwitterSemEval - парафразы в Twitter
  - TwitterURLCorpus - одинаковые URL в твитах
- **Важность для RAG**: Средняя - полезна для дедупликации документов
- **Что оценивается**: Точность определения семантической эквивалентности

##### 6. **Reranking (Переранжирование)**
- **Описание**: Улучшение порядка результатов поиска после первичной выдачи
- **Метрика**: MAP (Mean Average Precision)
- **Примеры датасетов**:
  - AskUbuntu - переранжирование ответов на технические вопросы
  - StackOverflow - улучшение порядка ответов
  - SciDocs - научные документы
- **Важность для RAG**: Высокая - улучшает качество финальной выдачи
- **Что оценивается**: Способность модели улучшать первоначальное ранжирование

##### 7. **Summarization (Оценка суммаризации)**
- **Описание**: Оценка качества автоматически созданных резюме
- **Метрика**: Spearman correlation с человеческими оценками
- **Примеры датасетов**:
  - SummEval - оценка суммаризации новостных статей
- **Важность для RAG**: Низкая - специфичная задача
- **Что оценивается**: Корреляция машинных оценок с человеческими

##### 8. **BitextMining (Извлечение параллельных текстов)**
- **Описание**: Поиск переводов одного и того же текста на разных языках
- **Метрика**: F1-score
- **Примеры датасетов**:
  - Tatoeba - параллельные предложения на множестве языков
  - BUCC - извлечение параллельных предложений из сопоставимых корпусов
- **Важность для RAG**: Низкая для монолингвальных систем, высокая для мультиязычных
- **Что оценивается**: Способность модели связывать эквивалентные тексты на разных языках

#### 3.1.2. Колонки MTEB Leaderboard

Таблица лидеров MTEB содержит следующие колонки, которые помогают оценить модели по различным критериям:

##### Основные метрики производительности:

1. **Average (Средний балл)**
   - Среднее арифметическое всех 8 категорий задач
   - Шкала: 0-100
   - Ключевой показатель общей производительности модели
   - Используется для сравнения моделей между собой

2. **Retrieval**
   - Средний результат по всем датасетам информационного поиска
   - Наиболее важная метрика для RAG-систем
   - Измеряет способность находить релевантные документы

3. **STS (Semantic Textual Similarity)**
   - Средний результат по задачам оценки семантического сходства
   - Важна для понимания близости текстов

4. **Classification**
   - Средняя точность классификации по всем датасетам
   - Показатель качества для задач категоризации

5. **Clustering**
   - Средний V-measure по датасетам кластеризации
   - Важна для организации документов

6. **PairClassification**
   - Средняя точность по задачам бинарной классификации пар
   - Полезна для определения дубликатов

7. **Reranking**
   - Средний MAP по задачам переранжирования
   - Критична для улучшения результатов поиска

8. **Summarization**
   - Корреляция с человеческими оценками суммаризации
   - Специфичная метрика

9. **BitextMining**
   - F1-score по задачам поиска параллельных текстов
   - Важна для мультиязычных систем

##### Дополнительные характеристики:

10. **Model Size (Размер модели)**
    - Количество параметров модели в миллионах/миллиардах
    - Пример: 560M, 1.2B, 7B
    - Важен для оценки требований к ресурсам

11. **Memory Usage (Использование памяти)**
    - Объем RAM, необходимый для загрузки модели
    - Измеряется в GB
    - Критичен для выбора hardware

12. **Embedding Dimensions (Размерность векторов)**
    - Размер выходного вектора
    - Типичные значения: 384, 768, 1024, 1536, 3072
    - Влияет на скорость поиска и требования к хранению

13. **Max Sequence Length (Максимальная длина последовательности)**
    - Максимальное количество токенов, которое может обработать модель
    - Типичные значения: 512, 2048, 8192, 32768
    - Критичен для работы с длинными документами

14. **Languages (Языки)**
    - Количество поддерживаемых языков
    - Может быть указано конкретное число или "multilingual"

15. **License (Лицензия)**
    - Тип лицензии: MIT, Apache 2.0, Commercial, и т.д.
    - Важен для коммерческого использования

16. **Release Date (Дата релиза)**
    - Когда модель была опубликована
    - Помогает отслеживать актуальность модели

#### 3.1.3. Топ-5 моделей по MTEB (общий рейтинг, январь 2025):

| Модель | Average | Retrieval | STS | Classification | Clustering | PairClass | Reranking | Summ | BitextMining |
|--------|---------|-----------|-----|----------------|------------|-----------|-----------|------|--------------|
| voyage-large-2-instruct | 68.3 | 55.7 | 86.4 | 74.5 | 52.1 | 88.9 | 60.2 | 31.4 | 87.9 |
| SFR-Embedding-2_R | 67.6 | 60.0 | 85.1 | 73.2 | 51.8 | 87.4 | 59.8 | 30.9 | 86.3 |
| text-embedding-3-large | 64.6 | 55.4 | 81.7 | 75.4 | 49.5 | 85.9 | 58.9 | 30.8 | 85.5 |
| BGE-M3 | 64.0 | 56.9 | 83.3 | 69.8 | 50.2 | 86.2 | 59.5 | 30.5 | 84.8 |
| multilingual-e5-large-instruct | 63.4 | 51.8 | 84.7 | 71.2 | 48.9 | 85.1 | 58.2 | 30.1 | 83.7 |

#### 3.1.4. Интерпретация результатов MTEB

**Как использовать результаты MTEB для выбора модели:**

1. **Для RAG-систем**: Приоритизируйте колонку **Retrieval**
   - Это прямой показатель эффективности поиска документов
   - Для production систем стремитесь к результату > 55
   - Модели с Retrieval > 60 считаются excellent для RAG

2. **Для семантического анализа**: Смотрите на **STS** (Semantic Textual Similarity)
   - Высокие значения (>80) указывают на точное понимание семантики
   - Важно для задач определения похожести документов

3. **Для мультиязычных систем**: Проверяйте **BitextMining**
   - Показатель > 80 говорит о хорошей кросс-языковой поддержке
   - Критично для перевода и многоязычного поиска

4. **Для гибридных систем**: Учитывайте **Reranking**
   - Высокие значения (>58) улучшат качество финальной выдачи
   - Особенно важно для многоэтапных RAG-пайплайнов

5. **Общий показатель Average**: Используйте как starting point
   - Не полагайтесь только на него
   - Всегда проверяйте конкретные метрики для вашего use case

**Важные замечания:**

- **Размер модели vs качество**: Большие модели не всегда лучше
  - BGE-M3 (1.2B) может уступать меньшим моделям на специфичных задачах
  - Выбирайте на основе требований к скорости и ресурсам

- **Длина контекста**: Проверяйте Max Sequence Length
  - Модель с высоким Average но коротким контекстом может не подойти
  - Для документов >2048 токенов нужны специализированные модели

- **Языковая специфика**: Общие результаты могут не отражать качество для конкретного языка
  - Всегда проверяйте результаты для вашего языка
  - Используйте специализированные бенчмарки (например, для русского)

**Пример выбора на основе MTEB:**

```
Задача: Корпоративный RAG-поиск на русском и английском языках

Шаг 1: Фильтруем по языкам → multilingual models
Шаг 2: Проверяем Retrieval → минимум 55+
Шаг 3: Смотрим на Max Sequence Length → нужно 2048+
Шаг 4: Сравниваем финалистов:
  - BGE-M3: Retrieval 56.9, длинный контекст (8192), multilingual ✓
  - multilingual-e5-large-instruct: Retrieval 51.8, средний контекст (512) ✗

Выбор: BGE-M3 лучше соответствует требованиям
```

### 3.2. Специализированные бенчмарки

#### **BEIR (Benchmarking IR)**
Фокус на задачах информационного поиска:
- **Лидеры**: BGE-M3, E5-mistral-7b-instruct
- **Метрика**: nDCG@10

#### **MIRACL (Multilingual IR)**
Оценка кросс-языкового поиска на 18 языках:
- **Лидеры**: mContriever-msmarco, BGE-M3
- **Важно**: Результаты сильно варьируются по языкам

### 3.3. Бенчмарки для русского языка

| Модель | Русский (MTEB) | Примечание |
|--------|---------------|------------|
| multilingual-e5-large | 61.2 | Лучший баланс |
| LaBSE | 58.7 | Хорошо для перевода |
| rubert-tiny2 | 57.3 | Оптимизирован для русского |
| sbert-large-nlu-ru | 56.8 | Специализированная модель |

## 4. Размеры контекстного окна и обработка документов

### 4.1. Классификация по размеру контекста

#### **Короткий контекст (до 512 токенов)**
- **Применение**: Короткие запросы, FAQ, метаданные
- **Модели**: SBERT, all-MiniLM-L6-v2, multilingual-e5-base
- **≈ Размер текста**: 300-400 слов

#### **Средний контекст (512-2048 токенов)**
- **Применение**: Параграфы, небольшие статьи
- **Модели**: BGE-large, E5-large
- **≈ Размер текста**: 1000-1500 слов

#### **Длинный контекст (2048-8192 токенов)**
- **Применение**: Полные документы, научные статьи
- **Модели**: BGE-M3, text-embedding-3-large, jina-embeddings-v2-base
- **≈ Размер текста**: 4000-6000 слов

#### **Сверхдлинный контекст (8192+ токенов)**
- **Применение**: Книги, технические мануалы
- **Модели**: 
  - Jina-embeddings-v3 (до 8192)
  - NV-Embed-v2 (до 32768)
  - voyage-large-2-instruct (до 16000)
- **≈ Размер текста**: 6000+ слов

### 4.2. Стратегии работы с большими документами

#### **1. Chunking (Разбиение)**
```python
# Пример стратегии разбиения
chunk_size = 512  # токенов
overlap = 50      # перекрытие между чанками

# Рекомендации:
# - Для технических текстов: chunk_size=1000-1500
# - Для нарративных текстов: chunk_size=500-800
# - Overlap: 10-20% от chunk_size
```

#### **2. Hierarchical Embeddings**
- Создание embeddings на разных уровнях: документ → раздел → параграф
- Модели: SPECTER2, BGE-M3 с поддержкой ColBERT

#### **3. Late Chunking**
- Обработка всего документа, затем разбиение embeddings
- Сохранение контекстуальной информации

## 5. Практические рекомендации по выбору

### 5.1. Дерево принятия решений

```
1. Языковые требования?
   ├── Только английский → BGE-large-en, E5-large
   └── Мультиязычность
       ├── 2-5 языков → Специализированные модели
       └── 10+ языков → multilingual-e5, BGE-M3

2. Размер документов?
   ├── < 512 токенов → all-MiniLM-L6-v2, E5-small
   ├── 512-2048 → BGE-large, E5-large
   └── > 2048 → BGE-M3, text-embedding-3-large

3. Ресурсные ограничения?
   ├── Минимальные → all-MiniLM-L6-v2 (80MB)
   ├── Средние → BGE-base (400MB)
   └── Без ограничений → BGE-M3, E5-mistral-7b

4. Требования к качеству?
   ├── Максимальное → voyage-large-2, text-embedding-3-large
   ├── Оптимальное → BGE-large, E5-large
   └── Достаточное → all-MiniLM-L6-v2
```

### 5.2. Рекомендации по сценариям использования

#### **Корпоративный поиск по документам**
- **Модель**: BGE-M3 или text-embedding-3-large
- **Причины**: Длинный контекст, высокое качество, мультиязычность

#### **Чат-бот с базой знаний**
- **Модель**: multilingual-e5-large или BGE-large
- **Причины**: Баланс качества и скорости

#### **Семантический поиск в e-commerce**
- **Модель**: E5-base или all-MiniLM-L6-v2
- **Причины**: Высокая скорость, достаточное качество для коротких текстов

#### **Научный поиск**
- **Модель**: SPECTER2 или SciBERT (для английского)
- **Причины**: Специализация на научных текстах

## 6. Метрики оценки качества для RAG

### 6.1. Offline метрики

- **MRR (Mean Reciprocal Rank)**: Позиция первого релевантного документа
- **Recall@k**: Доля найденных релевантных документов в топ-k
- **nDCG (Normalized Discounted Cumulative Gain)**: Учет порядка результатов

### 6.2. Online метрики

- **Answer Relevance**: Релевантность финального ответа
- **Context Precision**: Точность извлеченного контекста
- **Faithfulness**: Соответствие ответа извлеченному контексту

## 7. Инструменты и библиотеки

### 7.1. Фреймворки для работы с embeddings

#### **Пример 1: Sentence-Transformers (базовое использование)**

```python
from sentence_transformers import SentenceTransformer

# Загрузка модели
model = SentenceTransformer('multilingual-e5-large')

# Создание embeddings
texts = ["Пример текста на русском", "Example text in English"]
embeddings = model.encode(texts)

print(f"Размерность вектора: {embeddings.shape}")
```

#### **Пример 2: LangChain**

```python
from langchain.embeddings import HuggingFaceEmbeddings

# Инициализация модели через LangChain
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# Создание embeddings
text = "Текст для векторизации"
vector = embeddings.embed_query(text)
```

#### **Пример 3: LlamaIndex**

```python
from llama_index.embeddings import HuggingFaceEmbedding

# Загрузка модели
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5")

# Получение embeddings
embeddings = embed_model.get_text_embedding("Sample text")
```

#### **Пример 4: Qwen3-Embedding-8B (SOTA модель)**

**Qwen3-Embedding-8B** — лидирующая модель в MTEB multilingual leaderboard (70.58) с поддержкой 100+ языков, контекстом до 32k токенов и размерностью эмбеддингов до 4096.

##### Вариант A: Sentence-Transformers (рекомендуется)

```python
# Requires: transformers>=4.51.0, sentence-transformers>=2.7.0
from sentence_transformers import SentenceTransformer

# Загрузка модели
model = SentenceTransformer("Qwen/Qwen3-Embedding-8B")

# Для лучшей производительности рекомендуется использовать flash_attention_2
# model = SentenceTransformer(
#     "Qwen/Qwen3-Embedding-8B",
#     model_kwargs={"attn_implementation": "flash_attention_2", "device_map": "auto"},
#     tokenizer_kwargs={"padding_side": "left"},
# )

# Запросы и документы
queries = [
    "Какая столица Китая?",
    "Объясни гравитацию",
]
documents = [
    "Столица Китая — Пекин.",
    "Гравитация — это сила притяжения между телами. Она придает вес физическим объектам.",
]

# Создание embeddings
# Для запросов используем prompt "query" для улучшения качества поиска
query_embeddings = model.encode(queries, prompt_name="query")
document_embeddings = model.encode(documents)

# Вычисление сходства (косинусное)
similarity = model.similarity(query_embeddings, document_embeddings)
print(similarity)
# Результат: tensor([[0.7493, 0.0751], [0.0880, 0.6318]])
```

##### Вариант B: Transformers (низкоуровневое API)

```python
# Requires: transformers>=4.51.0
import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel

def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """Извлечение последнего токена для получения embedding"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[
            torch.arange(batch_size, device=last_hidden_states.device), 
            sequence_lengths
        ]

def get_detailed_instruct(task_description: str, query: str) -> str:
    """Создание инструкции для улучшения качества поиска"""
    return f'Instruct: {task_description}\nQuery: {query}'

# Определение задачи (инструкция помогает модели лучше понять контекст)
task = 'Для поискового запроса найди релевантные отрывки, которые отвечают на запрос'

# Подготовка запросов с инструкциями
queries = [
    get_detailed_instruct(task, 'Какая столица Китая?'),
    get_detailed_instruct(task, 'Объясни гравитацию')
]

# Документы не требуют инструкций
documents = [
    "Столица Китая — Пекин.",
    "Гравитация — это сила притяжения между телами."
]

input_texts = queries + documents

# Загрузка модели и токенизатора
tokenizer = AutoTokenizer.from_pretrained(
    'Qwen/Qwen3-Embedding-8B', 
    padding_side='left'
)
model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-8B')

# Для лучшей производительности (GPU):
# model = AutoModel.from_pretrained(
#     'Qwen/Qwen3-Embedding-8B',
#     attn_implementation="flash_attention_2",
#     torch_dtype=torch.float16
# ).cuda()

# Токенизация
max_length = 8192  # Максимальная длина контекста
batch_dict = tokenizer(
    input_texts,
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt",
)
batch_dict = batch_dict.to(model.device)

# Получение embeddings
with torch.no_grad():
    outputs = model(**batch_dict)
    embeddings = last_token_pool(
        outputs.last_hidden_state, 
        batch_dict['attention_mask']
    )

# Нормализация векторов
embeddings = F.normalize(embeddings, p=2, dim=1)

# Вычисление сходства между запросами и документами
scores = (embeddings[:2] @ embeddings[2:].T)
print(scores.tolist())
# [[0.749, 0.075], [0.088, 0.632]]
```

##### Вариант C: vLLM (высокопроизводительный inference)

```python
# Requires: vllm>=0.8.5
import torch
from vllm import LLM

def get_detailed_instruct(task_description: str, query: str) -> str:
    return f'Instruct: {task_description}\nQuery: {query}'

task = 'Для поискового запроса найди релевантные отрывки'

queries = [
    get_detailed_instruct(task, 'Какая столица Китая?'),
    get_detailed_instruct(task, 'Объясни гравитацию')
]

documents = [
    "Столица Китая — Пекин.",
    "Гравитация — это сила притяжения между телами."
]

input_texts = queries + documents

# Загрузка модели через vLLM
model = LLM(model="Qwen/Qwen3-Embedding-8B", task="embed")

# Получение embeddings
outputs = model.embed(input_texts)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])

# Вычисление сходства
scores = (embeddings[:2] @ embeddings[2:].T)
print(scores.tolist())
```

##### Практические рекомендации для Qwen3-Embedding-8B:

1. **Использование инструкций**: Рекомендуется создавать кастомные инструкции для специфичных задач — это улучшает качество на 1-5%
2. **Мультиязычность**: Пишите инструкции на английском языке для лучших результатов, даже если документы на другом языке
3. **Длинные документы**: Модель поддерживает до 32k токенов, что идеально для работы с полными документами
4. **Гибкая размерность**: Можно настроить размерность выходных векторов от 32 до 4096 для оптимизации скорости/качества
5. **Flash Attention**: Используйте `flash_attention_2` для ускорения и экономии памяти на GPU

### 7.2. Векторные базы данных

- **Производственные**: Pinecone, Weaviate, Qdrant, Milvus
- **Для разработки**: ChromaDB, FAISS
- **Гибридный поиск**: Elasticsearch + векторный поиск

## 8. Тренды и будущее развитие

### 8.1. Актуальные направления

1. **Instruction-tuned embeddings**: Модели, понимающие инструкции для улучшения поиска
2. **Matryoshka Embeddings**: Адаптивная размерность векторов
3. **Late Interaction Models**: ColBERT-подход для улучшения качества
4. **Domain-specific models**: Специализированные модели для конкретных областей

### 8.2. Emerging подходы

- **Контрастивное обучение**: Улучшение различения похожих документов
- **Multi-vector representations**: Несколько векторов для одного документа
- **Learned indices**: Обучаемые индексы вместо традиционных

## Заключение

Выбор подходящей embedding-модели для RAG-системы требует балансирования множества факторов. Начните с оценки ваших требований по языковой поддержке, размеру документов и доступным ресурсам. Используйте результаты бенчмарков как отправную точку, но обязательно проводите тестирование на ваших данных.

## Полезные ресурсы

1. **MTEB Leaderboard**: https://huggingface.co/spaces/mteb/leaderboard
2. **Sentence-Transformers документация**: https://www.sbert.net/
3. **Papers with Code - Text Embeddings**: https://paperswithcode.com/task/text-embeddings
4. **LangChain Embeddings Guide**: https://python.langchain.com/docs/modules/data_connection/text_embedding/
5. **Cohere Embed Playground**: https://dashboard.cohere.com/playground/embed
6. **OpenAI Embeddings Guide**: https://platform.openai.com/docs/guides/embeddings

## Практическое задание

1. Выберите 3 модели из разных категорий (малая, средняя, большая)
2. Создайте тестовый датасет из 100 пар "вопрос-документ" на вашем языке
3. Оцените качество поиска используя метрики MRR и Recall@5
4. Измерьте скорость обработки и потребление памяти
5. Проанализируйте trade-offs между качеством и производительностью
